---
layout: post
title:  "Stable Diffusion 3 Medium を ComfyUI で使ってみる"
date:   2024-06-20 00:00:00 +0900
categories: プログラム
tags:
- python
- 機械学習
- StableDiffusion
- ComfyUI
---
[Stable Diffusion 3 Medium がリリース][release] されて、[ローカルで試してみました][sd3] が、一枚生成するたびに 12 分もかかると気軽に試せません。しかも WebUI もない・・・と思っていましたが、モデルのページには以下の記述があることを見落としていました。

`For local or self-hosted use, we recommend ComfyUI for inference.`

どうやら ComfyUI というものがあるらしいです。Stable Diffusion WebUI よりも最新のモデルへの対応が早いらしく、人気が出ているそうです。今回初めて知ったので、環境を作ってみます。

なお、UI 自体は WebUI です。そのため、ポート転送さえできれば他の PC で処理を行って、操作はブラウザ経由で行うことが可能です。


### 準備
GPU を使えるように設定済みのコンテナを使います。

{% highlight bash %}
docker run -it --gpus=all --rm -v /home/work:/work nvcr.io/nvidia/cuda:12.1.0-base-ubuntu22.04 /bin/bash
{% endhighlight %}

必要なパッケージをインストールします。
{% highlight bash %}
apt update
apt install -y git python3-pip libgl1-mesa-dev libglib2.0-0
{% endhighlight %}

pip で pytorch をインストールします。
{% highlight bash %}
pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
{% endhighlight %}

ComfyUI のリポジトリをクローンし、必要なライブラリをインストールします。
{% highlight bash %}
git clone https://github.com/comfyanonymous/ComfyUI.git

cd ComfyUI
pip install –r requirements.txt
{% endhighlight %}

### ComfyUI Manager
ComfyUI だけでも動作するのですが、ほぼ必須ともいえる拡張機能の「ComfyUI Manager」をあわせて導入します。先ほどクローンした ComfyUI のディレクトリ内で以下コマンドを実行します。

{% highlight bash %}
pushd custom_nodes/
git clone https://github.com/ltdrdata/ComfyUI-Manager.git
popd
{% endhighlight %}

これで ComfyUI を起動すれば、自動的に ComfyUI Manager が認識されるはずです。


### 起動
クローンした ComfyUI のディレクトリで以下コマンドを実行し、起動します。

{% highlight bash %}
python3 main.py
{% endhighlight %}

しばらく待つと、以下のように `To see the GUI go to: http://127.0.0.1:8188` と表示されるので、ブラウザで表示された URL を開いてみましょう。

![start][img1]


ブラウザには、このように箱を線でつなげた図と、右下にメニューが表示されると思います。右下のメニューの一番下に「Manager」と「Share」のボタンがあれば、ComfyUI Manager の導入も問題なくできています。

![ui][img2]

### Stable Diffusion 3 Medium の実行
[モデル][release] のページで Files and versions より Comfy_example_workflows フォルダを開きます。ここにある json ファイルが ComfyUI の Workflow を定義するファイルになります。試しに `comfy_example_workflows_sd3_medium_example_workflow_basic.json` をダウンロードします。

処理に必要なモデルもダウンロードします。モデルのページから、以下のファイルをダウンロードしておきます。

+ sd3_medium.safetensors
+ text_encoders/clip_g.safetensors
+ text_encoders/clip_l.safetensors
+ text_encoders/t5xxl_fp8_e4m3fn.safetensors

json ファイルはブラウザからアクセスできる場所ならどこでも OK です。`sd3_medium.safetensors` は `ComfyUI/models/chekpoints` へ、`text_encoders` 配下の 3 ファイルは `ComfyUI/models/clips` へ保存します。このディレクトリ配下でないと、UI から認識されないようです。

ダウンロードできたら ComfyUI の右下のメニューから「Load」をクリックします。ファイル選択ダイアログが出るので、先ほどダウンロードした json ファイルを指定すると、Workflow が表示されると思います。

表示された Workflow はそのままだと実行できません。まず、モデルを選びます。左上、`Load Checkpoint` で `sd3_medium.safetensors` が表示されていることを確認します。

![checkpoint][img3]

もし表示されていない場合は、モデル名が表示されている欄をクリックすると一覧が表示されるので、そこから選択します。

![checkpoint][img4]


その下にある「Input」の枠内は、まず `Seed` の `controll_after_generation` の初期値が `fixed` (固定値) になっているので、`randomize` に変更します。これで、生成するたびに違うシード値になります。

![seed][img5]

その横にはプロンプトの入力欄があるので、ここは必要に応じて変更します。カンマ区切りで単語を並べるのではなく、文章で指定したほうがうまく出力できるようですね。(テキストエンコーダ次第かもしれませんが)

![prompt][img6]

とりあえずここまで設定すれば、あとは右下のメニューから `Queue Prompt` をクリックすると生成が始まります。

画面上部には緑のバーで進捗を示していて、実行中のノードには緑の枠が付くので、どの辺まで処理が進んでいるのか見ることができます。

![running][img7]

結果は右端の `Output` に表示されます。

![output][img8]

一枚生成するのに 40 秒程度。パラメータがいろいろありますが、それらの関係が図で見えるので、どこを変えたらどこに影響があるというのがわかりやすい気がしますね。



[release]:https://ja.stability.ai/blog/stable-diffusion-3-medium
[sd3]:{% post_url 2024/2024-06-13-sd3 %}

[img1]:/assets/images/2024/06/ss-20240620-01.png
[img2]:/assets/images/2024/06/ss-20240620-02.png
[img3]:/assets/images/2024/06/ss-20240620-03.png
[img4]:/assets/images/2024/06/ss-20240620-04.png
[img5]:/assets/images/2024/06/ss-20240620-05.png
[img6]:/assets/images/2024/06/ss-20240620-06.png
[img7]:/assets/images/2024/06/ss-20240620-07.png
[img8]:/assets/images/2024/06/ss-20240620-08.png

